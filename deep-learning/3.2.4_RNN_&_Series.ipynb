{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MP449NkJA-Rs"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sensioai/blog/blob/master/034_rnn_intro/rnn_intro.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3-S_OBKA-Rz"
      },
      "source": [
        "# Redes Neuronales Recurrentes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cL9zdccaA-R0"
      },
      "source": [
        "En clases hemos cubierto  temas relacionados con `redes neuronales`, sin embargo en todos los ejemplos y aplicaciones hemos usado siempre la misma `arquitectura` de red neuronal: el `Perceptrón Multicapa` (MLP), un modelo del que hemos hablado extensivamente.\n",
        "\n",
        "![](https://www.researchgate.net/profile/Mohamed_Zahran6/publication/303875065/figure/fig4/AS:371118507610123@1465492955561/A-hypothetical-example-of-Multilayer-Perceptron-Network.png)\n",
        "\n",
        "En este modelo tenemos una serie de capas, cada capa formada por un número determinado de `perceptrones` (o neuronas artificiales, la unidad básica que forma las `redes neuronales` de la que también hemos hablado en gran cantidad en posts anteriores). Si bien este modelo es muy útil y potente, lo hemos utilizado por ejemplo para clasificar imágenes, existen otras aplicaciones para las cuales esta arquitectura se ve limitada. Una familia de aplicaciones en las que nuestro `MLP` no será muy efectivo es aquellas en las que tratemos con `datos secuenciales`, como por ejemplo `series temporales` o aplicaciones de `lenguaje` en las que nuestros datos están organizados en secuencias de longitud variable. E"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "fTCbmVrVA-R1"
      },
      "source": [
        "## Definición"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "bEYa4dZGA-R2"
      },
      "source": [
        "En una red neuronal *feed forward* la información solo fluye en una dirección, desde la capa de entrada hasta la salida. Nuestro `MLP` es un ejemplo de este tipo de redes. Una `red recurrente` es muy similar a un `MLP` en el que además tenemos conexiones apuntando hacia detrás.\n",
        "\n",
        "![](https://miro.medium.com/max/2544/1*aIT6tmnk3qHpStkOX3gGcQ.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "MeoSdZdLA-R3"
      },
      "source": [
        "En la imágen anterior tienes un ejemplo de la `RNN` más sencilla que existe, en la que sólo tenemos una entrada, una neurona en la capa oculta y una salida. En el caso del `MLP` calcularíamos la salida con la siguiente fórmula\n",
        "\n",
        "$$ \\mathbf{y} = \\mathbf{W}_y \\mathbf{h} = \\mathbf{W}_y f(\\mathbf{W}_x \\mathbf{x}) $$\n",
        "\n",
        "Sin embargo, en una `RNN` además de las entradas que ya conocemos, $x$, introduciremos las salidas de la capa oculta en el instante anterior, $h$, como entradas adicionales, las cuales irán multiplicadas por su propia matriz de pesos\n",
        "\n",
        "$$ \\mathbf{y}_t = \\mathbf{W}_y \\mathbf{h}_t = \\mathbf{W}_y f(\\mathbf{W}_x \\mathbf{x}_t + \\mathbf{W}_h \\mathbf{h}_{t-1}) $$\n",
        "\n",
        "En los ejemplos que hemos visto anteriormente, cada muestra en el dataset estaba caracterizado por un número determinado de características (por ejemplo, para clasificar imágenes, cada entrada correspondía al valor de cada píxel de la imagen). Sin embargo, al trabajar con datos secuenciales, nuestro dataset estará formado por secuencias de elementos, cada uno de ellos caracterizado por un número determinado de características (para seguir con el ejemplo de imágenes, podrías considerar un vídeo en el cada elemento sería un frame, una imagen). Denotaremos a cada elemento de la secuencia como $\\mathbf{x}_t$ donde $\\mathbf{x}$ es el vector de características del elemento en la secuencia $t$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "Gnq-VbPwA-R5"
      },
      "source": [
        "## Tipos de RNNs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "iE10Hi_OA-R8"
      },
      "source": [
        "Esta nueva arquitectura abre un nuevo mundo de posibilidades en el que podemos llevar a cabo nuevas tareas dependiendo de la configuración de entradas/salidas que utilicemos\n",
        "\n",
        "![](https://i.stack.imgur.com/b4sus.jpg)\n",
        "\n",
        "La tipología *one-to-one* corresponde al `MLP`, puedes considerarlo como una secuencia de longitud 1, el resto de tipologías nos permiten tareas como clasificación de secuencias o regresión a un valor (*many-to-one*) o regresión a varios valores futuros (*many-to-many*). ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3nRqvdCA-R9"
      },
      "source": [
        "## Entrenando RNNs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zf7F_SwPA-R9"
      },
      "source": [
        "Para entrenar `RNNs` utilizaremos exactamente el mismo algoritmo que hemos utilizado hasta ahora: el algoritmo de *bakpropagation*. Ahora, sin embargo, hay que tener en cuenta que podemos tener gradientes fluyendo desde cualquiera de las salidas de la red neuronal, por lo que en un mismo paso de actualización es posible actualizar los pesos con (potencialmente) muchos gradientes distintos. Esto puede resultar en ciertos problemas durante el entrenamiento de los que hablaremos más adelante.\n",
        "\n",
        "![](https://s3.ap-south-1.amazonaws.com/techleer/191.png)\n",
        "\n",
        "Obviamente, en los frameworks de `redes neuronales` que ya conocemos esto ya está implementado por lo que no tenemos que preocuparnos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtLJL2rJA-R-"
      },
      "source": [
        "## Predicción de series temporales"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v66GoN6OA-R_"
      },
      "source": [
        "Muy bien, en este punto ya sabemos qué es una `red neuronal recurrente` así que vamos a aplicarlo a una de sus principales aplicaciones que también no servirá para asentar los conceptos explicados anteriormente. Vamos a empezar generando una colección de series temporales de datos sintéticos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-08-27T11:15:06.933489Z",
          "start_time": "2020-08-27T11:15:06.835445Z"
        },
        "id": "fQyHftOdA-R_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def generate_time_series(batch_size, n_steps):\n",
        "    freq1, freq2, offsets1, offsets2 = np.random.rand(4, batch_size, 1)\n",
        "    time = np.linspace(0, 1, n_steps)\n",
        "    series = 0.5 * np.sin((time - offsets1) * (freq1 * 10 + 10))  #   wave 1\n",
        "    series += 0.2 * np.sin((time - offsets2) * (freq2 * 20 + 20)) # + wave 2\n",
        "    series += 0.1 * (np.random.rand(batch_size, n_steps) - 0.5)   # + noise\n",
        "    return series[..., np.newaxis].astype(np.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-08-27T11:15:06.981489Z",
          "start_time": "2020-08-27T11:15:06.934489Z"
        },
        "id": "wGEu0f-oA-SB"
      },
      "outputs": [],
      "source": [
        "n_steps = 50\n",
        "series = generate_time_series(10000, n_steps + 1)\n",
        "X_train, y_train = series[:7000, :n_steps], series[:7000, -1]\n",
        "X_valid, y_valid = series[7000:9000, :n_steps], series[7000:9000, -1]\n",
        "X_test, y_test = series[9000:, :n_steps], series[9000:, -1]\n",
        "X_train.shape, y_train.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "saFfs3tXA-SC"
      },
      "source": [
        "Nuestro objetivo será el de predecir el último valor de la serie temporal a partir de todos los anteriores. De ser capaces de llevar a cabo esta tarea podríamos predecir, por ejemplo, la evolución de usuarios en una aplicación o web, el precio de las acciones de una empresa en bolsa, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-08-27T11:15:09.146885Z",
          "start_time": "2020-08-27T11:15:06.982492Z"
        },
        "code_folding": [
          2
        ],
        "id": "IYqh29IZA-SD"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_series(series, y=None, y_pred=None, y_pred_std=None, x_label=\"$t$\", y_label=\"$x$\"):\n",
        "  r, c = 3, 5\n",
        "  fig, axes = plt.subplots(nrows=r, ncols=c, sharey=True, sharex=True, figsize=(20, 10))\n",
        "  for row in range(r):\n",
        "    for col in range(c):\n",
        "        plt.sca(axes[row][col])\n",
        "        ix = col + row*c\n",
        "        plt.plot(series[ix, :], \".-\")\n",
        "        if y is not None:\n",
        "            plt.plot(range(len(series[ix, :]), len(series[ix, :])+len(y[ix])), y[ix], \"bx\", markersize=10)\n",
        "        if y_pred is not None:\n",
        "            plt.plot(range(len(series[ix, :]), len(series[ix, :])+len(y_pred[ix])), y_pred[ix], \"ro\")\n",
        "        if y_pred_std is not None:\n",
        "            plt.plot(range(len(series[ix, :]), len(series[ix, :])+len(y_pred[ix])), y_pred[ix] + y_pred_std[ix])\n",
        "            plt.plot(range(len(series[ix, :]), len(series[ix, :])+len(y_pred[ix])), y_pred[ix] - y_pred_std[ix])\n",
        "        plt.grid(True)\n",
        "        plt.hlines(0, 0, 100, linewidth=1)\n",
        "        plt.axis([0, len(series[ix, :])+len(y[ix]), -1, 1])\n",
        "        if x_label and row == r - 1:\n",
        "          plt.xlabel(x_label, fontsize=16)\n",
        "        if y_label and col == 0:\n",
        "          plt.ylabel(y_label, fontsize=16, rotation=0)\n",
        "  plt.show()\n",
        "\n",
        "plot_series(X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "uwIGSBTyA-SD"
      },
      "source": [
        "### Predicción *Naive*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "czQOF7yVA-SD"
      },
      "source": [
        "Un modelo muy simple y que puede funcionar sorprendentemente bien en algunas ocasiones es uno que siempre prediga el último valor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-08-27T11:15:09.542501Z",
          "start_time": "2020-08-27T11:15:09.147887Z"
        },
        "hidden": true,
        "id": "W5ZnudBGA-SE"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "y_pred = X_test[:,-1]\n",
        "mean_squared_error(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-08-27T11:15:11.227496Z",
          "start_time": "2020-08-27T11:15:09.543510Z"
        },
        "hidden": true,
        "id": "Vh0IoIynA-SE"
      },
      "outputs": [],
      "source": [
        "y_pred = X_test[:,-1]\n",
        "plot_series(X_test, y_test, y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "wBYAtRRdA-SF"
      },
      "source": [
        "Como puedes ver este modelo simple es bastante potente a la hora de predecir un único valor a partir de todos los anteriores."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExAFFF14A-SF"
      },
      "source": [
        "### Perceptrón Multicapa"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5oNAPqLA-SF"
      },
      "source": [
        "En el caso de que todas las secuencias tengan la misma longitud (lo cual no siempre es el caso) podríamos utilizar un `MLP` como los que ya conocemos para la tarea de regresión."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-08-27T11:15:11.572979Z",
          "start_time": "2020-08-27T11:15:11.228495Z"
        },
        "code_folding": [
          3
        ],
        "id": "9mwM67QUA-SF"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class TimeSeriesDataset(Dataset):\n",
        "  def __init__(self, X, y=None, train=True):\n",
        "    self.X = X\n",
        "    self.y = y\n",
        "    self.train = train\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.X)\n",
        "\n",
        "  def __getitem__(self, ix):\n",
        "    if self.train:\n",
        "      return torch.from_numpy(self.X[ix]), torch.from_numpy(self.y[ix])\n",
        "    return torch.from_numpy(self.X[ix])\n",
        "\n",
        "dataset = {\n",
        "    'train': TimeSeriesDataset(X_train, y_train),\n",
        "    'eval': TimeSeriesDataset(X_valid, y_valid),\n",
        "    'test': TimeSeriesDataset(X_test, y_test, train=False)\n",
        "}\n",
        "\n",
        "dataloader = {\n",
        "    'train': DataLoader(dataset['train'], shuffle=True, batch_size=64),\n",
        "    'eval': DataLoader(dataset['eval'], shuffle=False, batch_size=64),\n",
        "    'test': DataLoader(dataset['test'], shuffle=False, batch_size=64)\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-08-27T11:15:11.588978Z",
          "start_time": "2020-08-27T11:15:11.573978Z"
        },
        "code_folding": [
          5
        ],
        "id": "830BXk5WA-SG"
      },
      "outputs": [],
      "source": [
        "class MLP(torch.nn.Module):\n",
        "  def __init__(self, n_in=50, n_out=1):\n",
        "    super().__init__()\n",
        "    self.fc = torch.nn.Linear(n_in, n_out)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x.view(x.shape[0], -1)\n",
        "    x = self.fc(x)\n",
        "    return x\n",
        "\n",
        "mlp = MLP()\n",
        "mlp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-08-27T11:19:15.469054Z",
          "start_time": "2020-08-27T11:19:15.449038Z"
        },
        "code_folding": [
          3
        ],
        "id": "Hlpam8MBA-SG"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def fit(model, dataloader, epochs=10):\n",
        "    model.to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "    criterion = torch.nn.MSELoss()\n",
        "    bar = tqdm(range(1, epochs+1))\n",
        "    for epoch in bar:\n",
        "        model.train()\n",
        "        train_loss = []\n",
        "        for batch in dataloader['train']:\n",
        "            X, y = batch\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            y_hat = model(X)\n",
        "            loss = criterion(y_hat, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss.append(loss.item())\n",
        "        model.eval()\n",
        "        eval_loss = []\n",
        "        with torch.no_grad():\n",
        "            for batch in dataloader['eval']:\n",
        "                X, y = batch\n",
        "                X, y = X.to(device), y.to(device)\n",
        "                y_hat = model(X)\n",
        "                loss = criterion(y_hat, y)\n",
        "                eval_loss.append(loss.item())\n",
        "        bar.set_description(f\"loss {np.mean(train_loss):.5f} val_loss {np.mean(eval_loss):.5f}\")\n",
        "\n",
        "def predict(model, dataloader):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        preds = torch.tensor([]).to(device)\n",
        "        for batch in dataloader:\n",
        "            X = batch\n",
        "            X = X.to(device)\n",
        "            pred = model(X)\n",
        "            preds = torch.cat([preds, pred])\n",
        "        return preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-08-27T11:19:16.953605Z",
          "start_time": "2020-08-27T11:19:15.557019Z"
        },
        "id": "ArH3hdOwA-SG"
      },
      "outputs": [],
      "source": [
        "fit(mlp, dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-08-27T11:24:08.643958Z",
          "start_time": "2020-08-27T11:24:07.041243Z"
        },
        "id": "a0aJ1tPWA-SH"
      },
      "outputs": [],
      "source": [
        "y_pred = predict(mlp, dataloader['test'])\n",
        "plot_series(X_test, y_test, y_pred.cpu().numpy())\n",
        "mean_squared_error(y_test, y_pred.cpu())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-Qc_pBjA-SH"
      },
      "source": [
        "Nuestro `preceptrón` es capaz de predecir con mejor precisión el último valor en una serie temporal a partir del resto. Vamos ahora a ver cómo hacer lo mismo con una `red recurrente`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pm8_DwppA-SI"
      },
      "source": [
        "### Red Recurrente Simple"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QopkyLZ5A-SI"
      },
      "source": [
        "En `Pytorch` tenemos disponible la capa `torch.nn.RNN` que implementa la arquitectura de red neuronal recurrente que hemos presentado anteriormente. A esta capa le podemos pasar como parámetros, entre otros, el número de entradas, el número de neuronas en la capa oculta, el número de capa ocultas, etc. Poco a poco iremos viendo las diferentes opciones. Vamos a empezar con una `RNN` simple que acepta un solo valor a la entrada, una capa oculta de una sola neurona, y un valor a la salida (igual a la figura siguiente)\n",
        "\n",
        "![](https://miro.medium.com/max/2544/1*aIT6tmnk3qHpStkOX3gGcQ.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-08-27T11:24:43.315247Z",
          "start_time": "2020-08-27T11:24:43.297790Z"
        },
        "id": "qfLapXqlA-SI"
      },
      "outputs": [],
      "source": [
        "class SimpleRNN(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.rnn = torch.nn.RNN(input_size=1, hidden_size=1, num_layers=1, batch_first=True)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x, h = self.rnn(x)\n",
        "    # solo queremos la última salida\n",
        "    return x[:,-1]\n",
        "\n",
        "rnn = SimpleRNN()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-08-27T11:24:46.663485Z",
          "start_time": "2020-08-27T11:24:43.422338Z"
        },
        "id": "kpfOc464A-SJ"
      },
      "outputs": [],
      "source": [
        "fit(rnn, dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-08-27T11:24:48.338714Z",
          "start_time": "2020-08-27T11:24:46.665390Z"
        },
        "id": "9NJ-JT87A-SJ"
      },
      "outputs": [],
      "source": [
        "y_pred = predict(rnn, dataloader['test'])\n",
        "plot_series(X_test, y_test, y_pred.cpu().numpy())\n",
        "mean_squared_error(y_test, y_pred.cpu())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEPqjeTUA-SK"
      },
      "source": [
        "Según parece nuestra `RNN` no funciona mejor que el perceptrón anterior. ¿Cómo es posible si hemos dicho que las `redes recurrentes` funcionan mejor para datos secuenciales? La respuesta está en el número de parámetros de cada modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-08-27T11:25:55.793275Z",
          "start_time": "2020-08-27T11:25:55.780223Z"
        },
        "id": "14VBWWcGA-SK"
      },
      "outputs": [],
      "source": [
        "# parámetros en el MLP\n",
        "\n",
        "mlp.fc.weight.shape, mlp.fc.bias.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DrUxUlnaA-SL"
      },
      "source": [
        "En el `MLP` tenemos 50 entradas conectadas a una salida, en total 50 parámetros más el bias, 51."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-08-27T11:26:16.569819Z",
          "start_time": "2020-08-27T11:26:16.564815Z"
        },
        "id": "ic2zrV2GA-SL"
      },
      "outputs": [],
      "source": [
        "# parámetros en la RNN simple\n",
        "\n",
        "rnn.rnn.weight_hh_l0.shape, rnn.rnn.weight_ih_l0.shape, rnn.rnn.bias_hh_l0.shape, rnn.rnn.bias_ih_l0.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UITSpvmfA-SL"
      },
      "source": [
        "En la `RNN` tenemos una entrada conecta a una neurona en la capa oculta (que al tener solo una capa oculta también es la salida), un peso más el bias, 2. Además, tenemos una conexión de la neurona en la capa oculta en un instante a la capa oculta en el instante siguiente, otro peso más el bias, 2 más. En total 4 parámetros contra los 51 del `MLP`. Necesitamos una `RNN` con más capacidad, por ejemplo aumentando el número de neuronas en la capa oculta. Para este caso, además, necesitaremos una capa lineal que nos de el último valor a partir de los 20 valores en la capa oculta."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-08-27T11:30:20.785691Z",
          "start_time": "2020-08-27T11:30:20.765667Z"
        },
        "id": "nS75WXG0A-SR"
      },
      "outputs": [],
      "source": [
        "class RNN(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.rnn = torch.nn.RNN(input_size=1, hidden_size=20, num_layers=1, batch_first=True)\n",
        "    self.fc = torch.nn.Linear(20, 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x, h = self.rnn(x)\n",
        "    # get the last output and apply linear layer\n",
        "    y = self.fc(x[:,-1])\n",
        "    return y\n",
        "\n",
        "rnn = RNN()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-08-27T11:30:52.314410Z",
          "start_time": "2020-08-27T11:30:52.308999Z"
        },
        "id": "pamQ_b-IA-SR"
      },
      "outputs": [],
      "source": [
        "# parámetros en la nueva RNN\n",
        "\n",
        "rnn.rnn.weight_hh_l0.shape, rnn.rnn.weight_ih_l0.shape, rnn.rnn.bias_hh_l0.shape, rnn.rnn.bias_ih_l0.shape, rnn.fc.weight.shape, rnn.fc.bias.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-08-27T11:29:45.659792Z",
          "start_time": "2020-08-27T11:29:42.045015Z"
        },
        "id": "K19XGwBtA-SR"
      },
      "outputs": [],
      "source": [
        "fit(rnn, dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-08-27T11:29:57.828939Z",
          "start_time": "2020-08-27T11:29:56.148089Z"
        },
        "id": "JIqttZyxA-SR"
      },
      "outputs": [],
      "source": [
        "y_pred = predict(rnn, dataloader['test'])\n",
        "plot_series(X_test, y_test, y_pred.cpu().numpy())\n",
        "mean_squared_error(y_test, y_pred.cpu())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Reu2nabKA-SS"
      },
      "source": [
        "Ahora sí, nuestra `RNN` tiene un poder similar al `MLP`. La diferencia es que el `MLP` recibe a la entrada todos los valores a la vez para generar la única salida, mientras que la `RNN` recibe los valores de 1 en 1, generando una salida en cada instante de las cuales nos quedamos con la última. Si nuestras secuencias fuesen de diferente longitud, no podríamos aplicar el `MLP` mientras que la `RNN` funcionaría sin problemas.\n",
        "\n",
        "![](https://www.simplilearn.com/ice9/free_resources_article_thumb/Fully_connected_Recurrent_Neural_Network.gif)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7OSc5BIA-SS"
      },
      "source": [
        "## Resumen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuB0mESPA-SS"
      },
      "source": [
        "Aquí hemos introducido una nueva arquitectura de `red neuronal` que nos va a permitir trabajar con datos secuenciales: la `red neuronal recurrente`. Esta arquitectura es muy similar a la del `Perceptrón Multicapa` , en la que en cada capa oculta no solo tenemos conexiones a las entradas de la capa anterior si no también a los valores de la misma capa oculta pero en el instante anterior. De esta manera seremos capaces de llevar a cabo tareas de regresión y clasificación con datos secuenciales como series temporales, lenguaje, vídeos, etc."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {
        "height": "calc(100% - 180px)",
        "left": "10px",
        "top": "150px",
        "width": "233.594px"
      },
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}